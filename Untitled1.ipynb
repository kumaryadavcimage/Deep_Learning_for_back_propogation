{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bfb862-ca7d-437b-8778-f00eb177b71b",
   "metadata": {},
   "source": [
    "#### Q1. What is the purpose of forward propagation in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ab89d-7a54-4038-8b7c-5165545aba67",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Forward propagation in a neural network is the process through which input data passes through the layers of the network to produce an output. It involves the following key steps:\n",
    "\n",
    "- Input to the Network: The input data is fed into the input layer.\n",
    "\n",
    "- Weighted Sum Calculation: Each neuron in a layer takes inputs from the previous layer, multiplies them by weights, adds a bias term, and computes a weighted sum.\n",
    "\n",
    "- Activation Function: This weighted sum is then passed through an activation function, introducing non-linearity into the model. The activation function helps the network learn and solve complex problems.\n",
    "\n",
    "- Output Generation: This process continues for all layers until the final output layer generates a result, which could be a classification, prediction, or some other form of output depending on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc80c81-c261-4ae6-bbe0-7f46cbf89f4b",
   "metadata": {},
   "source": [
    "#### Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb6c2e-120f-4b41-af1a-026cc5667a18",
   "metadata": {},
   "source": [
    "#### solve\n",
    "In a single-layer feedforward neural network (also called a perceptron), forward propagation involves simple mathematical operations that compute the output from the input, weights, and biases. Let's break it down step-by-step:\n",
    "\n",
    "Notation:\n",
    "\n",
    "- input vector: x =[x1,x2,....xn] (the features)\n",
    "- Weight vector: w=[w1,w2,...,wn] (weights associated with each input feature)\n",
    "- Bias: b (a constandt added to the weightd sum)\n",
    "- Activation function: f (a function applied to intoduce non-linearity, such as sigmoid, ReLu, etc.)\n",
    "- Output: y^ ( the prediction made by the neuron)\n",
    "\n",
    "Step-by-step Forward Propagation:\n",
    "\n",
    "- Weighted Sum Calculation: The input vector x is mulitiplied element-wise by the weight vector w, and the sum of these produrcts is computed. Additionally , the bias b is added. This is givern by:\n",
    "\n",
    "           z w.x + b = ∑(i=1 to n) wixi + b\n",
    "\n",
    "Where z is the weighted sum( or pre-activation value).\n",
    "\n",
    "- Activation Function: The weighted sum z is passed through an activation function f(z) to produce the final output:\n",
    "\n",
    "            y^ = f(z) = f(w * x + b)\n",
    "\n",
    "- Example Activation Functions:\n",
    "\n",
    "Sigmoid: f(z) = 1/1+e^ -z ( used in binary classification)\n",
    "\n",
    "ReLu (Rectified Linear Unit): f(z) = max(0,z) ( commonly used in deep networks)\n",
    "\n",
    "Linear: f(z) = z (often used for reagression tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbe75f-82b8-48a0-9de2-7451c22b6b95",
   "metadata": {},
   "source": [
    "#### Q3. How are activation functions used during forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e738b-8cab-44c5-9041-ae4b428114a6",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Activation functions are crucial during forward propagation in a neural network because they introduce non-linearity into the model. Without them, the network would behave like a simple linear model, no matter how many layers it has. Here's how activation functions are used during forward propagation:\n",
    "\n",
    "Role of Activation Functions:\n",
    "\n",
    "Non-linearity: Activation functions allow the network to model complex patterns by adding non-linearity. This helps the network capture relationships that a purely linear model would miss.\n",
    "\n",
    "Deciding Neuron Output: The activation function processes the neuron's weighted sum of inputs (plus bias) to determine whether the neuron should be \"activated\" or \"fired.\" This transformed value is passed as input to the next layer or used for the final output.\n",
    "\n",
    "Gradient Flow: During backpropagation (the training phase), activation functions influence how gradients flow through the network, affecting weight updates and learning.\n",
    "\n",
    "How Activation Functions are Used:\n",
    "\n",
    "After calculating the weighted sum z =w*x + b, the activation function f(z) is applied to compute the final output of the neuron. The activation function is used at each neuron in all hidden layers (and sometimes the output layer, depending on the task).\n",
    "\n",
    "Types of Activation Functions:\n",
    "\n",
    "-  Sigmoid (Logistic):       f(z) = 1/1+e^ -z\n",
    "\n",
    "Usage: Commonly used for binary classification problems.\n",
    "\n",
    "Range: Outputs values between 0 and 1.\n",
    "\n",
    "Effect: Maps the input to a probability-like output.\n",
    "\n",
    "Limitation: Can cause vanishing gradient problems during training for deep networks.\n",
    "\n",
    "- ReLU (Rectified Linear Unit):  f(z) = max(0,z)\n",
    "\n",
    "Usage: Popular in deep networks, especially convolutional neural networks (CNNs).\n",
    "\n",
    "Range: Outputs values from 0 to ∞.\n",
    "\n",
    "Effect: Efficient, simple, and helps avoid vanishing gradient issues.\n",
    "\n",
    "Limitation: Can result in \"dead neurons\" (neurons that never activate if they only receive negative inputs).\n",
    "\n",
    "- Leaky ReLU:   f(z) { z is if z>0 , az if z<=0 ( a is a small value, e.g., 0.01)\n",
    "\n",
    "Usage: Used to fix the issue of dead neurons in ReLU.\n",
    "\n",
    "Range: Outputs values from -∞ to ∞.\n",
    "\n",
    "Effect: Allows small negative values to flow through the network, keeping the gradient alive.\n",
    "\n",
    "- Tanh (Hyperbolic Tangent):   f(z) = e^z - e^ -z / e^z + e^ -z\n",
    "\n",
    "Usage: Used in hidden layers for tasks requiring values between -1 and 1.\n",
    "\n",
    "Range: Outputs values between -1 and 1.\n",
    "\n",
    "Effect: Similar to sigmoid but zero-centered, making gradient updates more balanced.\n",
    "\n",
    "Limitation: Like sigmoid, it can also suffer from vanishing gradients in deep networks.\n",
    "\n",
    "- Softmax:             f(zi) = e^zi / ∑ ( j=1 to n)  e^zi\n",
    "\n",
    "Usage: Typically used in the output layer for multi-class classification tasks.\n",
    "\n",
    "Range: Outputs a probability distribution where the sum of probabilities is 1.\n",
    "\n",
    "Effect: Turns raw output scores into probabilities for multi-class problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b69781-283e-468a-b229-6b69d1f250ab",
   "metadata": {},
   "source": [
    "#### Q4. What is the role of weights and biases in forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9bd11-0e6f-43b1-813d-b7e8a655031f",
   "metadata": {},
   "source": [
    "#### solve\n",
    "\n",
    "In forward propagation, weights and biases play a critical role in determining how the input data is transformed as it moves through the layers of a neural network. They control the strength of the connections between neurons and influence the output generated by each neuron. Here's a breakdown of their roles:\n",
    "\n",
    "a. Weights:\n",
    "\n",
    "Weights determine how much influence each input has on the neuron's output. They are the key parameters the network learns during training.\n",
    "\n",
    "Connection Strength: In a neural network, each input feature is multiplied by a corresponding weight. These weights represent the strength of the connection between neurons, controlling how much importance is given to each feature.\n",
    "\n",
    "Linear Transformation: The output of each neuron is based on a linear combination of inputs, where weights are the coefficients in this linear equation. Mathematically, for a single neuron:\n",
    "\n",
    "      z = w1x1 + w2x2 + ... +wnxn +b\n",
    "Where w1, w2,... wn are the weights, and x1, x2,... , xn are the inputs. \n",
    "\n",
    "Learning: During training, the neural network adjusts the weights to minimize the error in predictions. Correctly tuned weights help the model capture the underlying patterns in the data.\n",
    "\n",
    "Contribution to Model's Flexibility: Weights allow the network to learn complex patterns. Different sets of weights can represent different decision boundaries or transformations in the feature space.\n",
    "\n",
    "b. Biases:\n",
    "\n",
    "Biases allow the model to shift the activation function, providing additional flexibility in the model’s decision-making process.\n",
    "\n",
    "Offset for the Activation Function: Bias is a constant added to the weighted sum of inputs before applying the activation function. It helps shift the activation function so that the network can model more complex relationships. Mathematically:\n",
    "\n",
    "            z = w*x +b\n",
    "\n",
    "\n",
    "where b is the bias, and it ensures that even if all input values are zero, the neuron can still activate or produce a non-zero output.\n",
    "\n",
    "Improving Learning: Biases are crucial when certain patterns in data are not centered around zero. Without biases, the neuron would be forced to produce zero output when all inputs are zero, limiting the network’s learning ability.\n",
    "\n",
    "Adjustment During Training: Like weights, biases are also adjusted during training to minimize the error. They are treated as trainable parameters that shift the output of the linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7c8b8-cb75-49fa-a1c4-a74229277865",
   "metadata": {},
   "source": [
    "#### Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc730131-01c9-4ffd-b12d-124d25602396",
   "metadata": {},
   "source": [
    "#### solve\n",
    "The softmax function is commonly applied in the output layer of a neural network, especially in classification tasks involving multiple classes (multi-class classification). Its primary purpose is to transform the raw output scores (also called logits) into a probability distribution over the different classes. Here’s why this is important:\n",
    "\n",
    "Purpose of the Softmax Function:\n",
    "\n",
    "- Converting Logits to Probabilities: The raw outputs (logits) from the last layer of a neural network are not easily interpretable as probabilities because they can be any real number (positive, negative, or zero). The softmax function converts these logits into probabilities, where:\n",
    "\n",
    "Each probability represents the model’s confidence that a given input belongs to each specific class.\n",
    "\n",
    "The sum of the probabilities for all classes is 1, which satisfies the properties of a probability distribution.\n",
    "\n",
    "- Facilitating Decision Making: By converting the outputs into probabilities, the softmax function makes it easy to select the most likely class. The class with the highest probability becomes the model’s predicted output. This is crucial in multi-class classification tasks, such as object recognition or text classification.\n",
    "\n",
    "How Softmax Works:\n",
    "\n",
    "Given a vector of raw z = [z1,z2,...,zn] for n classes, the softmax function calculates the probability for each class i as:\n",
    "                                                                                                       \n",
    "                P(y=i/z) = e^zi / ∑(j=1 to n) e^zj\n",
    "                                                                                                       \n",
    "Where:\n",
    "\n",
    "e^zi is the exponentiation of the raw score zi for class i.\n",
    "\n",
    "∑(j=1 to n) e^zj is the sum of the exponentiated scores for all classes. \n",
    "\n",
    "This formula ensures that:\n",
    "\n",
    "All probabilities are positive.\n",
    "\n",
    "The probabilities for all classes sum to 1, forming a valid probability distribution. \n",
    "\n",
    "Key Properties of Softmax:\n",
    "\n",
    "Range: The output of the softmax function is always between 0 and 1, making it suitable for probability representation.\n",
    "\n",
    "Normalization: Softmax normalizes the output logits, so they are transformed into a probability distribution, where the most likely class has the highest probability.\n",
    "\n",
    "Exponentiation Effect: By using exponentiation, the softmax function amplifies differences between larger and smaller values, making the largest score even more dominant (i.e., the class with the highest raw score will have the highest probability).\n",
    "\n",
    "                 Example:\n",
    "Suppose a neural \n",
    "                 network predicts raw scores (logits) for 3 classes as:\n",
    "\n",
    "                           z = [2.0,1.0,0.1]\n",
    "\n",
    "The softmax function will compute the probabilities as follows:\n",
    "\n",
    "Exponentiate the logits:\n",
    "\n",
    "                 e^ 2.0 = 7.389, e^ 1.0 = 2.718, e^ 0.1 = 1.105\n",
    "\n",
    "sum the exponentiated values: \n",
    "\n",
    "                 7.389 + 2.718 + 1.105  = 11.212\n",
    "\n",
    "Calculate the probabilities for each class:\n",
    "\n",
    "                 P(class 1) =  7.389 / 11.212 ≈ 0.659\n",
    "\n",
    "                 P(class 2) =  2.718 /  11.212 ≈ 0.242\n",
    "\n",
    "                 P(class 2) =  1.105 / 11.212 ≈ 0.099\n",
    "\n",
    "Now, the output probabilities are  [0.659,0.242,0.099]. The model will predict class 1, as it has the highest probability (65.9%).\n",
    "\n",
    "Why Use Softmax in Multi-Class Classification:\n",
    "\n",
    "Multi-Class Prediction: Softmax is ideal for multi-class classification, where each input can belong to one of many possible classes. It ensures that the outputs are mutually exclusive and the probabilities sum to 1, enabling easy decision-making.\n",
    "\n",
    "Interpretable Probabilities: Since the output of softmax is a probability distribution, it provides interpretable results. The model can express how confident it is in each class, which is valuable in many applications (e.g., medical diagnosis, where confidence matters).                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05456bf9-b805-4842-942b-52b709a582fb",
   "metadata": {},
   "source": [
    "#### Q6. What is the purpose of backward propagation in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a58379-6858-441a-9a36-d92472ec3ef0",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Backward propagation (or backpropagation) is a key algorithm in neural networks used during the training phase to update the weights and biases of the network based on the error (or loss) between the predicted output and the actual target. The purpose of backpropagation is to minimize this error by iteratively adjusting the network’s parameters, enabling the network to learn from data.\n",
    "\n",
    "Key Purposes of Backpropagation:\n",
    "\n",
    "- Error Minimization:\n",
    "\n",
    "Backpropagation adjusts the weights and biases in the network to minimize the difference between the predicted output and the true target, which is measured by a loss function (e.g., mean squared error, cross-entropy loss).\n",
    "\n",
    "The ultimate goal is to reduce the loss and improve the accuracy of the network's predictions by continuously tweaking the network's parameters.\n",
    "\n",
    "- Efficient Gradient Computation:\n",
    "\n",
    "Backpropagation efficiently computes the gradients (partial derivatives of the loss function with respect to the weights and biases) using the chain rule of calculus. These gradients indicate how much each weight and bias contributes to the overall error.\n",
    "\n",
    "The algorithm propagates the error from the output layer back through the network to calculate the gradients for all the layers, hence the name \"backward propagation.\"\n",
    "\n",
    "- Parameter Update:\n",
    "\n",
    "After computing the gradients, backpropagation uses an optimization algorithm (like gradient descent) to update the weights and biases in the direction that reduces the loss.\n",
    "\n",
    "Each parameter is updated by moving it in the direction of the negative gradient (i.e., in the direction that reduces the error) by a certain step size, controlled by the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06159688-c6db-40c4-9245-9a322b3bc249",
   "metadata": {},
   "source": [
    "#### Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169b622-2ad1-43ba-b5ba-798bc433a0d6",
   "metadata": {},
   "source": [
    "#### solve\n",
    "\n",
    "In a single-layer feedforward neural network (also known as a perceptron), backward propagation is used to compute how the loss (error) depends on the weights and biases, and it updates these parameters accordingly.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "- Input Layer: The input x (a vector of features).\n",
    "- Weights and Biases: The weight matrix W and bias vector b associated with the connections between the input and the output.\n",
    "- Activation Function: A function f(z) applied to the weighted sum of inputs (e.g., sigmoid, ReLU).\n",
    "- Loss Function: Measures the difference between the predicted output y^ and the actual target y.\n",
    "\n",
    "Steps for Backward Propagation in a Single-Layer Neural Network:\n",
    "a. Forward Propagation:\n",
    "\n",
    "First, in forward propagation, we compute the output of the network using the current weights and biases.\n",
    "\n",
    "Weighted Sum: Compute the weighted sum of inputs and bias (also called logits):\n",
    "\n",
    "           z w*x + b\n",
    "\n",
    "Activation: Apply an activation function f(z) to get the output 𝑦^ :\n",
    "\n",
    "          y^ = f(z)\n",
    "\n",
    " Loss Calculation:\n",
    "Next, compute the loss using a loss function L( y^ ,y), where y is the true target, and 𝑦^ is the predicted output from the network. Common loss functions include:\n",
    "\n",
    "Mean Squared Error (MSE) for regression:\n",
    "\n",
    "          L = 1/2 (y^ - y) ^2\n",
    "\n",
    "Cross-Entropy Loss for classification:\n",
    "\n",
    "         L = -(ylog(y^) + (1-y) log(1-y^))\n",
    "\n",
    "Backward Propagation:\n",
    "\n",
    "Now, backpropagation computes the gradients of the loss function with respect to the weights W and biases b, using the chain rule.\n",
    "\n",
    "(a) Gradient of the Loss with Respect to the Output:\n",
    "\n",
    "First, compute the gradient of the loss L with respect to the predicted output 𝑦^.\n",
    "\n",
    "For Mean Squared Error (MSE):    ∂L/ ∂y^ = y^ - y\n",
    "\n",
    "For Cross-Entropy Loss (with sigmoid activation):\n",
    "\n",
    "                  ∂L/ ∂y^ = - y/y^ + (1-y / 1-y^)\n",
    "\n",
    "\n",
    "(b) Gradient of the Output with Respect to the Weighted Sum:                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b5f95-c35d-4b5f-915c-6d730d4723aa",
   "metadata": {},
   "source": [
    "#### Q8. Can you explain the concept of the chain rule and its application in backward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42788189-2aa1-46b2-b40c-93e52adc4277",
   "metadata": {},
   "source": [
    "#### solve\n",
    "The chain rule is a fundamental concept in calculus that helps us compute the derivative of a composite function, i.e., a function made up of other functions. In the context of backward propagation in neural networks, the chain rule is essential for calculating the gradients of the loss function with respect to the network's parameters (weights and biases) efficiently.\n",
    "\n",
    "Chain Rule in Calculus:\n",
    "\n",
    "If you have two function g(x) and f(g(x)), the chain rule allows you to compute the derivative of the composition f(g(x)) wiht respect to x. It states:\n",
    "\n",
    "               d f(g(x)) / dx = f'(g(x)).g'(x)\n",
    "\n",
    "In words, the derivative of the outer fundtion f with respect to s is the derivative of f with respect to g(x), multiplied by the derivative of g(x) with respect to x.\n",
    "\n",
    "Chain Rule in Backward Propagation:\n",
    "\n",
    "In neural networks, backward propagation computes the gradients of the loss function with respect to the parameters (weights and biases) by applying the chain rule repeatedly. This is because the output of each layer is a function of the outputs of the previous layer, forming a composite structure of functions.\n",
    "\n",
    "Here’s a breakdown of how the chain rule applies to backpropagation:\n",
    "\n",
    "a. Neural Network as a Composite Function:\n",
    "\n",
    "Consider a simple feedforward neural network with one hidden layer:\n",
    "\n",
    "Input Layer: The input to the network is x.\n",
    "\n",
    "Hidden Layere: The hidden layer computes z1 = w1*x + b1 and applies an activation function a1 = f1(z1).\n",
    "\n",
    "Output Layer: The output layer computes z2 = w2 * a1 +b2, and applies another activation funtion a2 = f2(z2), resulting in the final output y^ = a2.\n",
    "\n",
    "Now, the loss function L(y^,y) measures the difference between the network's output y^ and the true target y. The loss depends on the weights w1 and w2 , biaes b1 and b2, and input x. To update the weights and biases, we need to compte the partial derivateves of the loss with respect to each  parameter, i.e., ∂L / ∂w1 ' ∂L / ∂w2 , and similarly for the biases.\n",
    "\n",
    "Applying the Chain Rule:\n",
    "\n",
    "Since each layer's output is a function of the previous layer’s output, and ultimately the loss is a function of the output, we need to apply the chain rule to propagate the gradients backward through the layers. Here’s how it works:\n",
    "\n",
    "Step 1: Gradient of Loss with Respect to Output y^: The first step is to compute the gradient of the loss with respect to the final output y^, i.e., ∂L / ∂y^. This is straightforward and depends on the chosen loss function. For example, for mean squared error (MSE):\n",
    "\n",
    "              ∂L / ∂y^ = y^ -y\n",
    "\n",
    "step 2: Gradient of Ouput y^ with Respect to the Previous Layere (Activation Funtion): The next step is to sompute the grasient of y^ with respect to z2 , the weighted input to the output layer. This depends on the activation funtion applied at the output layer. If we use the sigmoid activation function f2(z2) = 1 / 1+ e^ -z2, its derivative is:\n",
    "\n",
    "         ∂a2 / ∂z2 = a2(1-a2)\n",
    "\n",
    "so, using the chain rule:\n",
    "\n",
    "         ∂L/∂z2 = ∂L/ ∂a2 . ∂a2 / ∂z2\n",
    "\n",
    "step 3: Gradinet with Respect to weights W2 and Biases b2: Now, we compute how the loss changes with respect to the weights W2 and bias b2 in the output layere. Since z2 = W2*a1 + b2, the gradients are:\n",
    "\n",
    "        ∂z2/ ∂W2 = a1 and ∂z2 / ∂b2 = 1\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "       ∂L / ∂W2 = (∂L / ∂z2) * (∂z2 / ∂W2) = δ2*a1\n",
    "\n",
    "       ∂L/∂b2 = (∂L/∂z2) * (∂z2/∂b2) = δ2\n",
    "\n",
    "Where δ2 = ∂L / ∂z2 is the error term at the output layer.\n",
    "\n",
    "Step 4: Backpropagate to Hidden Layer: The next step is to backpropagate the error to the hissen layer. THis involves computing the gradinet of z2 ith respect to a1, and then the graient of the loss wiht respect to a1, using the dhain rule again:\n",
    "\n",
    "        ∂L / ∂a1 = (∂L / ∂z2) * (∂z2 / ∂a1) = δ2*w2\n",
    "\n",
    "Now, apply the chain rule to compute the graidient with respect to the input to the hidden layer, z1:\n",
    "\n",
    "        ∂L / ∂z1 = (∂L / ∂a1) * (∂a1 / ∂z1)\n",
    "\n",
    "If the activation function in the hidden layer is sigmoid, the derivative is:\n",
    "\n",
    "        ∂L / ∂z1 = a1(1-a1)\n",
    "\n",
    "so:\n",
    "\n",
    "        ∂L / ∂z1 = δ1 = (δ2*W2)* a1(1-a1)\n",
    "\n",
    "Step 5: Gradinet with Respect to weights W1 and Biases b1: Finally, compute the gradinets with respect to the weights W1 and bias b1:\n",
    "\n",
    "        ∂L / ∂w1 =  δ1*x\n",
    "\n",
    "         ∂L / ∂b1 =  δ1\n",
    "\n",
    "Thus, by applying the chain rule at each step, the gradients of the loss with respect to all parameters (weights and biases) are computed, allowing for their update during training.\n",
    "\n",
    "Generalized Chain Rule for Backpropagation:\n",
    "\n",
    "The process described above applies to each layer in the neural network. The chain rule allows the error to be propagated backward through the network, layer by layer. For eah layer i, the gradinet of the loss with respect to the weights and baises is calculated as:\n",
    "\n",
    "Compute the error term 𝛿𝑖 for the current layer.\n",
    "\n",
    "Use the chain rule to propagate the gradient from the current layer to the previous layer.\n",
    "\n",
    "Repeat until the input layer is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6b96c-0e57-41d0-aceb-fd3e90442028",
   "metadata": {},
   "source": [
    "#### Q9. What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09b89a-4293-4022-8143-53a97649c158",
   "metadata": {},
   "source": [
    "#### solve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88e721-d8e9-4237-a605-60598c246aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
